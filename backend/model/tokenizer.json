{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 3, "word_counts": "{\"hola\": 1, \"buenas\": 1, \"adi\\u00f3s\": 1}", "word_docs": "{\"hola\": 1, \"buenas\": 1, \"adi\\u00f3s\": 1}", "index_docs": "{\"1\": 1, \"2\": 1, \"3\": 1}", "index_word": "{\"1\": \"hola\", \"2\": \"buenas\", \"3\": \"adi\\u00f3s\"}", "word_index": "{\"hola\": 1, \"buenas\": 2, \"adi\\u00f3s\": 3}"}}